{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJf6BSSZn303",
        "outputId": "2c2da15f-8706-4604-8997-f49a6aca23b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqex-mmB2oqk"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tensorflow import keras\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9sCEtYLk2oqn"
      },
      "outputs": [],
      "source": [
        "file_path = '/content/input_variables.xlsx'\n",
        "df = pd.read_excel(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Filter the DataFrame to exclude rows containing =-inf values\n",
        "df_filtered = df[~df.astype(str).apply(lambda row: any(\"=-inf\" in cell for cell in row), axis=1)]\n",
        "\n",
        "# Write the filtered DataFrame back to a new Excel file\n",
        "df_filtered.to_excel(\"filtered_file.xlsx\", index=False)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IGQesRy9WTkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the Excel file into a DataFrame\n",
        "df = pd.read_excel(\"/content/filtered_file.xlsx\")\n",
        "\n",
        "# Remove rows with blank values\n",
        "df_filtered = df.dropna()\n",
        "\n",
        "# Write the filtered DataFrame back to a new Excel file\n",
        "df_filtered.to_excel(\"input.xlsx\", index=False)\n"
      ],
      "metadata": {
        "id": "eg0fDvbEbPtk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X = df_filtered.iloc[:, :5].values\n",
        "y = df_filtered.iloc[:, 5:].values\n"
      ],
      "metadata": {
        "id": "lO-rnFjf5hYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "# Define your data preprocessing pipeline\n",
        "scaler = RobustScaler()\n",
        "X = scaler.fit_transform(X)\n"
      ],
      "metadata": {
        "id": "w7BJ8e507YEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "id": "V7Ws5dZsE0ku",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c318e0b6-1974-47cd-899b-406218ed35fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9507, 5)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape\n"
      ],
      "metadata": {
        "id": "elhvfEVhEOIS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a2ea2e8-783f-4c11-c48a-04d75d9c93a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9507, 22)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tensorflow import keras\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Normalize the data (optional but often recommended for neural networks)\n",
        "\n",
        "\n",
        "# Split the data into training, validation, and testing sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.7, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
      ],
      "metadata": {
        "id": "RiwwUq162Jzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-addons\n"
      ],
      "metadata": {
        "id": "-_um_LYTfWpV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7057cd3f-b969-40d0-849c-887c0291929e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (611 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/611.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.0/611.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.8/611.8 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (24.0)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.23.0 typeguard-2.13.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, LeakyReLU\n",
        "from keras.optimizers import Nadam\n",
        "\n",
        "# Define your model class\n",
        "class MyModel(Sequential):\n",
        "    def __init__(self):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.add(LSTM(8, activation=LeakyReLU(alpha=0.01), return_sequences=True, trainable=False))\n",
        "        self.add(LSTM(8, activation=LeakyReLU(alpha=0.01), trainable=True))\n",
        "        self.add(Dense(22))\n",
        "\n",
        "# Instantiate your model\n",
        "model = MyModel()\n",
        "\n",
        "# Reshape the input data to include a timestep dimension\n",
        "X_train_reshaped = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
        "X_val_reshaped = X_val.reshape(X_val.shape[0], 1, X_val.shape[1])\n",
        "X_test_reshaped = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
        "\n",
        "# Load weights after calling the model\n",
        "model.build(input_shape=(None, 1, X_train.shape[1]))  # Build the model with input shape\n",
        "model.load_weights('/content/lstm_model_weights (4).h5', by_name=True)\n",
        "\n",
        "# Set the second layer to be trainable\n",
        "model.layers[1].trainable = True\n",
        "\n",
        "# Define the optimizer with the desired learning rate\n",
        "optimizer = Nadam(learning_rate=0.002)\n",
        "\n",
        "# Compile the model with the specified optimizer and loss function\n",
        "model.compile(optimizer=optimizer, loss=tf.keras.losses.Huber())\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train_reshaped, y_train, validation_data=(X_val_reshaped, y_val), epochs=300, batch_size=50, verbose=1)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "y_train_pred = model.predict(X_train_reshaped)\n",
        "y_val_pred = model.predict(X_val_reshaped)\n",
        "y_test_pred = model.predict(X_test_reshaped)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FDlNjC7Q4MyM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7f67a57d-8c49-4bbe-8150-62c86f0919eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n",
            "58/58 [==============================] - 6s 21ms/step - loss: 4.1220 - val_loss: 3.6785\n",
            "Epoch 2/300\n",
            "58/58 [==============================] - 0s 8ms/step - loss: 2.5966 - val_loss: 1.0870\n",
            "Epoch 3/300\n",
            "58/58 [==============================] - 0s 7ms/step - loss: 0.6716 - val_loss: 0.4782\n",
            "Epoch 4/300\n",
            "58/58 [==============================] - 0s 7ms/step - loss: 0.4310 - val_loss: 0.4169\n",
            "Epoch 5/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.4015 - val_loss: 0.4003\n",
            "Epoch 6/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.3826 - val_loss: 0.3755\n",
            "Epoch 7/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.3566 - val_loss: 0.3501\n",
            "Epoch 8/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.3354 - val_loss: 0.3333\n",
            "Epoch 9/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.3217 - val_loss: 0.3215\n",
            "Epoch 10/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.3125 - val_loss: 0.3128\n",
            "Epoch 11/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.3055 - val_loss: 0.3073\n",
            "Epoch 12/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.3014 - val_loss: 0.3031\n",
            "Epoch 13/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2984 - val_loss: 0.3014\n",
            "Epoch 14/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2964 - val_loss: 0.2988\n",
            "Epoch 15/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2944 - val_loss: 0.2954\n",
            "Epoch 16/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2909 - val_loss: 0.2929\n",
            "Epoch 17/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2893 - val_loss: 0.2917\n",
            "Epoch 18/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2876 - val_loss: 0.2956\n",
            "Epoch 19/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2874 - val_loss: 0.2890\n",
            "Epoch 20/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2859 - val_loss: 0.2887\n",
            "Epoch 21/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2857 - val_loss: 0.2893\n",
            "Epoch 22/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2858 - val_loss: 0.2877\n",
            "Epoch 23/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2853 - val_loss: 0.2898\n",
            "Epoch 24/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2848 - val_loss: 0.2872\n",
            "Epoch 25/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2847 - val_loss: 0.2883\n",
            "Epoch 26/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2843 - val_loss: 0.2871\n",
            "Epoch 27/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2844 - val_loss: 0.2886\n",
            "Epoch 28/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2840 - val_loss: 0.2872\n",
            "Epoch 29/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2844 - val_loss: 0.2857\n",
            "Epoch 30/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2831 - val_loss: 0.2861\n",
            "Epoch 31/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2834 - val_loss: 0.2856\n",
            "Epoch 32/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2825 - val_loss: 0.2852\n",
            "Epoch 33/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2826 - val_loss: 0.2849\n",
            "Epoch 34/300\n",
            "58/58 [==============================] - 1s 13ms/step - loss: 0.2824 - val_loss: 0.2845\n",
            "Epoch 35/300\n",
            "58/58 [==============================] - 1s 12ms/step - loss: 0.2820 - val_loss: 0.2846\n",
            "Epoch 36/300\n",
            "58/58 [==============================] - 1s 12ms/step - loss: 0.2821 - val_loss: 0.2849\n",
            "Epoch 37/300\n",
            "58/58 [==============================] - 1s 12ms/step - loss: 0.2819 - val_loss: 0.2841\n",
            "Epoch 38/300\n",
            "58/58 [==============================] - 0s 9ms/step - loss: 0.2815 - val_loss: 0.2915\n",
            "Epoch 39/300\n",
            "58/58 [==============================] - 1s 11ms/step - loss: 0.2839 - val_loss: 0.2839\n",
            "Epoch 40/300\n",
            "58/58 [==============================] - 1s 12ms/step - loss: 0.2812 - val_loss: 0.2858\n",
            "Epoch 41/300\n",
            "58/58 [==============================] - 1s 12ms/step - loss: 0.2812 - val_loss: 0.2844\n",
            "Epoch 42/300\n",
            "58/58 [==============================] - 1s 12ms/step - loss: 0.2810 - val_loss: 0.2861\n",
            "Epoch 43/300\n",
            "58/58 [==============================] - 0s 7ms/step - loss: 0.2818 - val_loss: 0.2874\n",
            "Epoch 44/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2813 - val_loss: 0.2837\n",
            "Epoch 45/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2808 - val_loss: 0.2835\n",
            "Epoch 46/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2805 - val_loss: 0.2833\n",
            "Epoch 47/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2803 - val_loss: 0.2830\n",
            "Epoch 48/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2802 - val_loss: 0.2862\n",
            "Epoch 49/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2816 - val_loss: 0.2840\n",
            "Epoch 50/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2815 - val_loss: 0.2836\n",
            "Epoch 51/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2794 - val_loss: 0.2834\n",
            "Epoch 52/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2809 - val_loss: 0.2875\n",
            "Epoch 53/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2818 - val_loss: 0.2832\n",
            "Epoch 54/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2800 - val_loss: 0.2891\n",
            "Epoch 55/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2829 - val_loss: 0.2827\n",
            "Epoch 56/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2800 - val_loss: 0.2857\n",
            "Epoch 57/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2802 - val_loss: 0.2836\n",
            "Epoch 58/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2798 - val_loss: 0.2844\n",
            "Epoch 59/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2823 - val_loss: 0.2830\n",
            "Epoch 60/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2807 - val_loss: 0.2823\n",
            "Epoch 61/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2794 - val_loss: 0.2827\n",
            "Epoch 62/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2790 - val_loss: 0.2844\n",
            "Epoch 63/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2806 - val_loss: 0.2824\n",
            "Epoch 64/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2793 - val_loss: 0.2826\n",
            "Epoch 65/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2798 - val_loss: 0.2888\n",
            "Epoch 66/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2798 - val_loss: 0.2821\n",
            "Epoch 67/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2789 - val_loss: 0.2817\n",
            "Epoch 68/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2789 - val_loss: 0.2815\n",
            "Epoch 69/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2783 - val_loss: 0.2814\n",
            "Epoch 70/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2796 - val_loss: 0.2856\n",
            "Epoch 71/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2789 - val_loss: 0.2808\n",
            "Epoch 72/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2773 - val_loss: 0.2810\n",
            "Epoch 73/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2782 - val_loss: 0.2817\n",
            "Epoch 74/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2773 - val_loss: 0.2807\n",
            "Epoch 75/300\n",
            "58/58 [==============================] - 1s 11ms/step - loss: 0.2778 - val_loss: 0.2809\n",
            "Epoch 76/300\n",
            "58/58 [==============================] - 0s 8ms/step - loss: 0.2781 - val_loss: 0.2802\n",
            "Epoch 77/300\n",
            "58/58 [==============================] - 0s 8ms/step - loss: 0.2773 - val_loss: 0.2800\n",
            "Epoch 78/300\n",
            "58/58 [==============================] - 1s 11ms/step - loss: 0.2771 - val_loss: 0.2828\n",
            "Epoch 79/300\n",
            "58/58 [==============================] - 0s 8ms/step - loss: 0.2767 - val_loss: 0.2801\n",
            "Epoch 80/300\n",
            "58/58 [==============================] - 1s 11ms/step - loss: 0.2770 - val_loss: 0.2808\n",
            "Epoch 81/300\n",
            "58/58 [==============================] - 1s 11ms/step - loss: 0.2780 - val_loss: 0.2796\n",
            "Epoch 82/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2763 - val_loss: 0.2795\n",
            "Epoch 83/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2766 - val_loss: 0.2808\n",
            "Epoch 84/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2772 - val_loss: 0.2795\n",
            "Epoch 85/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2762 - val_loss: 0.2806\n",
            "Epoch 86/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2765 - val_loss: 0.2799\n",
            "Epoch 87/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2763 - val_loss: 0.2833\n",
            "Epoch 88/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2774 - val_loss: 0.2816\n",
            "Epoch 89/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2768 - val_loss: 0.2822\n",
            "Epoch 90/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2782 - val_loss: 0.2793\n",
            "Epoch 91/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2762 - val_loss: 0.2789\n",
            "Epoch 92/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2763 - val_loss: 0.2799\n",
            "Epoch 93/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2759 - val_loss: 0.2796\n",
            "Epoch 94/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2763 - val_loss: 0.2791\n",
            "Epoch 95/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2758 - val_loss: 0.2787\n",
            "Epoch 96/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2754 - val_loss: 0.2784\n",
            "Epoch 97/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2757 - val_loss: 0.2785\n",
            "Epoch 98/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2752 - val_loss: 0.2805\n",
            "Epoch 99/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2775 - val_loss: 0.2791\n",
            "Epoch 100/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2763 - val_loss: 0.2783\n",
            "Epoch 101/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2749 - val_loss: 0.2787\n",
            "Epoch 102/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2745 - val_loss: 0.2890\n",
            "Epoch 103/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2785 - val_loss: 0.2784\n",
            "Epoch 104/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2749 - val_loss: 0.2789\n",
            "Epoch 105/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2745 - val_loss: 0.2787\n",
            "Epoch 106/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2747 - val_loss: 0.2784\n",
            "Epoch 107/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2744 - val_loss: 0.2786\n",
            "Epoch 108/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2748 - val_loss: 0.2779\n",
            "Epoch 109/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2745 - val_loss: 0.2797\n",
            "Epoch 110/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2751 - val_loss: 0.2777\n",
            "Epoch 111/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2744 - val_loss: 0.2776\n",
            "Epoch 112/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2746 - val_loss: 0.2774\n",
            "Epoch 113/300\n",
            "58/58 [==============================] - 1s 9ms/step - loss: 0.2740 - val_loss: 0.2780\n",
            "Epoch 114/300\n",
            "58/58 [==============================] - 1s 11ms/step - loss: 0.2741 - val_loss: 0.2774\n",
            "Epoch 115/300\n",
            "58/58 [==============================] - 0s 8ms/step - loss: 0.2742 - val_loss: 0.2819\n",
            "Epoch 116/300\n",
            "58/58 [==============================] - 0s 8ms/step - loss: 0.2744 - val_loss: 0.2776\n",
            "Epoch 117/300\n",
            "58/58 [==============================] - 0s 8ms/step - loss: 0.2736 - val_loss: 0.2775\n",
            "Epoch 118/300\n",
            "58/58 [==============================] - 1s 10ms/step - loss: 0.2750 - val_loss: 0.2881\n",
            "Epoch 119/300\n",
            "58/58 [==============================] - 1s 11ms/step - loss: 0.2796 - val_loss: 0.2795\n",
            "Epoch 120/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2744 - val_loss: 0.2783\n",
            "Epoch 121/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2735 - val_loss: 0.2769\n",
            "Epoch 122/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2735 - val_loss: 0.2776\n",
            "Epoch 123/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2734 - val_loss: 0.2773\n",
            "Epoch 124/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2737 - val_loss: 0.2772\n",
            "Epoch 125/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2740 - val_loss: 0.2792\n",
            "Epoch 126/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2743 - val_loss: 0.2790\n",
            "Epoch 127/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2747 - val_loss: 0.2822\n",
            "Epoch 128/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2736 - val_loss: 0.2771\n",
            "Epoch 129/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2733 - val_loss: 0.2781\n",
            "Epoch 130/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2743 - val_loss: 0.2782\n",
            "Epoch 131/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2743 - val_loss: 0.2777\n",
            "Epoch 132/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2732 - val_loss: 0.2771\n",
            "Epoch 133/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2742 - val_loss: 0.2776\n",
            "Epoch 134/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2736 - val_loss: 0.2788\n",
            "Epoch 135/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2733 - val_loss: 0.2768\n",
            "Epoch 136/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2733 - val_loss: 0.2778\n",
            "Epoch 137/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2726 - val_loss: 0.2766\n",
            "Epoch 138/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2732 - val_loss: 0.2775\n",
            "Epoch 139/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2736 - val_loss: 0.2768\n",
            "Epoch 140/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2733 - val_loss: 0.2804\n",
            "Epoch 141/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2753 - val_loss: 0.2769\n",
            "Epoch 142/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2730 - val_loss: 0.2775\n",
            "Epoch 143/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2732 - val_loss: 0.2767\n",
            "Epoch 144/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2726 - val_loss: 0.2766\n",
            "Epoch 145/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2723 - val_loss: 0.2776\n",
            "Epoch 146/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2727 - val_loss: 0.2788\n",
            "Epoch 147/300\n",
            "58/58 [==============================] - 0s 7ms/step - loss: 0.2737 - val_loss: 0.2784\n",
            "Epoch 148/300\n",
            "58/58 [==============================] - 0s 7ms/step - loss: 0.2736 - val_loss: 0.2768\n",
            "Epoch 149/300\n",
            "58/58 [==============================] - 1s 10ms/step - loss: 0.2734 - val_loss: 0.2781\n",
            "Epoch 150/300\n",
            "58/58 [==============================] - 1s 12ms/step - loss: 0.2732 - val_loss: 0.2816\n",
            "Epoch 151/300\n",
            "58/58 [==============================] - 1s 11ms/step - loss: 0.2733 - val_loss: 0.2864\n",
            "Epoch 152/300\n",
            "58/58 [==============================] - 1s 12ms/step - loss: 0.2755 - val_loss: 0.2770\n",
            "Epoch 153/300\n",
            "58/58 [==============================] - 1s 11ms/step - loss: 0.2731 - val_loss: 0.2778\n",
            "Epoch 154/300\n",
            "58/58 [==============================] - 0s 8ms/step - loss: 0.2732 - val_loss: 0.2789\n",
            "Epoch 155/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2729 - val_loss: 0.2766\n",
            "Epoch 156/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2725 - val_loss: 0.2774\n",
            "Epoch 157/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2733 - val_loss: 0.2767\n",
            "Epoch 158/300\n",
            "58/58 [==============================] - 1s 9ms/step - loss: 0.2732 - val_loss: 0.2764\n",
            "Epoch 159/300\n",
            "58/58 [==============================] - 1s 10ms/step - loss: 0.2727 - val_loss: 0.2814\n",
            "Epoch 160/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2746 - val_loss: 0.2787\n",
            "Epoch 161/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2739 - val_loss: 0.2765\n",
            "Epoch 162/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2729 - val_loss: 0.2764\n",
            "Epoch 163/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2721 - val_loss: 0.2792\n",
            "Epoch 164/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2741 - val_loss: 0.2777\n",
            "Epoch 165/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2725 - val_loss: 0.2766\n",
            "Epoch 166/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2725 - val_loss: 0.2781\n",
            "Epoch 167/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2731 - val_loss: 0.2765\n",
            "Epoch 168/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2722 - val_loss: 0.2764\n",
            "Epoch 169/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2722 - val_loss: 0.2791\n",
            "Epoch 170/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2733 - val_loss: 0.2787\n",
            "Epoch 171/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2732 - val_loss: 0.2770\n",
            "Epoch 172/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2722 - val_loss: 0.2792\n",
            "Epoch 173/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2726 - val_loss: 0.2770\n",
            "Epoch 174/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2722 - val_loss: 0.2764\n",
            "Epoch 175/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2720 - val_loss: 0.2769\n",
            "Epoch 176/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2723 - val_loss: 0.2773\n",
            "Epoch 177/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2725 - val_loss: 0.2781\n",
            "Epoch 178/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2720 - val_loss: 0.2765\n",
            "Epoch 179/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2723 - val_loss: 0.2770\n",
            "Epoch 180/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2717 - val_loss: 0.2778\n",
            "Epoch 181/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2715 - val_loss: 0.2777\n",
            "Epoch 182/300\n",
            "58/58 [==============================] - 0s 7ms/step - loss: 0.2729 - val_loss: 0.2781\n",
            "Epoch 183/300\n",
            "58/58 [==============================] - 0s 8ms/step - loss: 0.2726 - val_loss: 0.2763\n",
            "Epoch 184/300\n",
            "58/58 [==============================] - 1s 11ms/step - loss: 0.2723 - val_loss: 0.2770\n",
            "Epoch 185/300\n",
            "58/58 [==============================] - 1s 11ms/step - loss: 0.2725 - val_loss: 0.2763\n",
            "Epoch 186/300\n",
            "58/58 [==============================] - 1s 12ms/step - loss: 0.2719 - val_loss: 0.2766\n",
            "Epoch 187/300\n",
            "58/58 [==============================] - 1s 12ms/step - loss: 0.2733 - val_loss: 0.2760\n",
            "Epoch 188/300\n",
            "58/58 [==============================] - 0s 8ms/step - loss: 0.2715 - val_loss: 0.2770\n",
            "Epoch 189/300\n",
            "58/58 [==============================] - 0s 7ms/step - loss: 0.2720 - val_loss: 0.2766\n",
            "Epoch 190/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2715 - val_loss: 0.2818\n",
            "Epoch 191/300\n",
            "58/58 [==============================] - 0s 7ms/step - loss: 0.2728 - val_loss: 0.2765\n",
            "Epoch 192/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2723 - val_loss: 0.2784\n",
            "Epoch 193/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2726 - val_loss: 0.2762\n",
            "Epoch 194/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2713 - val_loss: 0.2777\n",
            "Epoch 195/300\n",
            "58/58 [==============================] - 0s 7ms/step - loss: 0.2727 - val_loss: 0.2760\n",
            "Epoch 196/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2716 - val_loss: 0.2797\n",
            "Epoch 197/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2725 - val_loss: 0.2762\n",
            "Epoch 198/300\n",
            "58/58 [==============================] - 1s 15ms/step - loss: 0.2718 - val_loss: 0.2844\n",
            "Epoch 199/300\n",
            "58/58 [==============================] - 0s 7ms/step - loss: 0.2729 - val_loss: 0.2767\n",
            "Epoch 200/300\n",
            "58/58 [==============================] - 1s 10ms/step - loss: 0.2715 - val_loss: 0.2758\n",
            "Epoch 201/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2729 - val_loss: 0.2762\n",
            "Epoch 202/300\n",
            "58/58 [==============================] - 1s 11ms/step - loss: 0.2715 - val_loss: 0.2769\n",
            "Epoch 203/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2716 - val_loss: 0.2766\n",
            "Epoch 204/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2719 - val_loss: 0.2755\n",
            "Epoch 205/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2716 - val_loss: 0.2756\n",
            "Epoch 206/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2720 - val_loss: 0.2783\n",
            "Epoch 207/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2721 - val_loss: 0.2785\n",
            "Epoch 208/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2730 - val_loss: 0.2757\n",
            "Epoch 209/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2714 - val_loss: 0.2765\n",
            "Epoch 210/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2720 - val_loss: 0.2761\n",
            "Epoch 211/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2716 - val_loss: 0.2766\n",
            "Epoch 212/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2720 - val_loss: 0.2780\n",
            "Epoch 213/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2724 - val_loss: 0.2765\n",
            "Epoch 214/300\n",
            "58/58 [==============================] - 1s 11ms/step - loss: 0.2720 - val_loss: 0.2759\n",
            "Epoch 215/300\n",
            "58/58 [==============================] - 1s 11ms/step - loss: 0.2709 - val_loss: 0.2777\n",
            "Epoch 216/300\n",
            "58/58 [==============================] - 1s 9ms/step - loss: 0.2719 - val_loss: 0.2761\n",
            "Epoch 217/300\n",
            "58/58 [==============================] - 1s 11ms/step - loss: 0.2708 - val_loss: 0.2769\n",
            "Epoch 218/300\n",
            "58/58 [==============================] - 1s 9ms/step - loss: 0.2714 - val_loss: 0.2765\n",
            "Epoch 219/300\n",
            "58/58 [==============================] - 1s 12ms/step - loss: 0.2711 - val_loss: 0.2761\n",
            "Epoch 220/300\n",
            "58/58 [==============================] - 0s 7ms/step - loss: 0.2712 - val_loss: 0.2767\n",
            "Epoch 221/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2712 - val_loss: 0.2755\n",
            "Epoch 222/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2721 - val_loss: 0.2779\n",
            "Epoch 223/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2714 - val_loss: 0.2757\n",
            "Epoch 224/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2710 - val_loss: 0.2759\n",
            "Epoch 225/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2716 - val_loss: 0.2765\n",
            "Epoch 226/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2710 - val_loss: 0.2777\n",
            "Epoch 227/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2717 - val_loss: 0.2764\n",
            "Epoch 228/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2713 - val_loss: 0.2759\n",
            "Epoch 229/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2719 - val_loss: 0.2770\n",
            "Epoch 230/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2711 - val_loss: 0.2757\n",
            "Epoch 231/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2719 - val_loss: 0.2775\n",
            "Epoch 232/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2720 - val_loss: 0.2755\n",
            "Epoch 233/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2710 - val_loss: 0.2763\n",
            "Epoch 234/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2708 - val_loss: 0.2775\n",
            "Epoch 235/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2718 - val_loss: 0.2756\n",
            "Epoch 236/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2707 - val_loss: 0.2775\n",
            "Epoch 237/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2714 - val_loss: 0.2781\n",
            "Epoch 238/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2713 - val_loss: 0.2758\n",
            "Epoch 239/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2711 - val_loss: 0.2756\n",
            "Epoch 240/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2708 - val_loss: 0.2840\n",
            "Epoch 241/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2717 - val_loss: 0.2784\n",
            "Epoch 242/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2714 - val_loss: 0.2776\n",
            "Epoch 243/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2709 - val_loss: 0.2760\n",
            "Epoch 244/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2704 - val_loss: 0.2818\n",
            "Epoch 245/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2726 - val_loss: 0.2796\n",
            "Epoch 246/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2716 - val_loss: 0.2788\n",
            "Epoch 247/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2714 - val_loss: 0.2755\n",
            "Epoch 248/300\n",
            "58/58 [==============================] - 0s 8ms/step - loss: 0.2713 - val_loss: 0.2759\n",
            "Epoch 249/300\n",
            "58/58 [==============================] - 1s 9ms/step - loss: 0.2710 - val_loss: 0.2753\n",
            "Epoch 250/300\n",
            "58/58 [==============================] - 0s 8ms/step - loss: 0.2703 - val_loss: 0.2762\n",
            "Epoch 251/300\n",
            "58/58 [==============================] - 1s 11ms/step - loss: 0.2711 - val_loss: 0.2763\n",
            "Epoch 252/300\n",
            "58/58 [==============================] - 0s 8ms/step - loss: 0.2708 - val_loss: 0.2754\n",
            "Epoch 253/300\n",
            "58/58 [==============================] - 1s 11ms/step - loss: 0.2710 - val_loss: 0.2771\n",
            "Epoch 254/300\n",
            "58/58 [==============================] - 1s 11ms/step - loss: 0.2720 - val_loss: 0.2763\n",
            "Epoch 255/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2714 - val_loss: 0.2777\n",
            "Epoch 256/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2706 - val_loss: 0.2754\n",
            "Epoch 257/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2707 - val_loss: 0.2771\n",
            "Epoch 258/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2726 - val_loss: 0.2774\n",
            "Epoch 259/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2715 - val_loss: 0.2759\n",
            "Epoch 260/300\n",
            "58/58 [==============================] - 0s 7ms/step - loss: 0.2703 - val_loss: 0.2755\n",
            "Epoch 261/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2715 - val_loss: 0.2770\n",
            "Epoch 262/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2707 - val_loss: 0.2776\n",
            "Epoch 263/300\n",
            "58/58 [==============================] - 0s 5ms/step - loss: 0.2724 - val_loss: 0.2760\n",
            "Epoch 264/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2714 - val_loss: 0.2768\n",
            "Epoch 265/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2710 - val_loss: 0.2761\n",
            "Epoch 266/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2708 - val_loss: 0.2752\n",
            "Epoch 267/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2707 - val_loss: 0.2755\n",
            "Epoch 268/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2710 - val_loss: 0.2751\n",
            "Epoch 269/300\n",
            "58/58 [==============================] - 0s 7ms/step - loss: 0.2704 - val_loss: 0.2802\n",
            "Epoch 270/300\n",
            "58/58 [==============================] - 0s 7ms/step - loss: 0.2714 - val_loss: 0.2763\n",
            "Epoch 271/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2705 - val_loss: 0.2764\n",
            "Epoch 272/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2708 - val_loss: 0.2751\n",
            "Epoch 273/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2704 - val_loss: 0.2776\n",
            "Epoch 274/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2703 - val_loss: 0.2765\n",
            "Epoch 275/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2715 - val_loss: 0.2790\n",
            "Epoch 276/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2715 - val_loss: 0.2753\n",
            "Epoch 277/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2703 - val_loss: 0.2761\n",
            "Epoch 278/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2706 - val_loss: 0.2771\n",
            "Epoch 279/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2716 - val_loss: 0.2755\n",
            "Epoch 280/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2702 - val_loss: 0.2752\n",
            "Epoch 281/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2705 - val_loss: 0.2759\n",
            "Epoch 282/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2703 - val_loss: 0.2751\n",
            "Epoch 283/300\n",
            "58/58 [==============================] - 1s 11ms/step - loss: 0.2699 - val_loss: 0.2773\n",
            "Epoch 284/300\n",
            "58/58 [==============================] - 0s 8ms/step - loss: 0.2701 - val_loss: 0.2756\n",
            "Epoch 285/300\n",
            "58/58 [==============================] - 1s 9ms/step - loss: 0.2701 - val_loss: 0.2757\n",
            "Epoch 286/300\n",
            "58/58 [==============================] - 1s 11ms/step - loss: 0.2704 - val_loss: 0.2767\n",
            "Epoch 287/300\n",
            "58/58 [==============================] - 0s 8ms/step - loss: 0.2699 - val_loss: 0.2909\n",
            "Epoch 288/300\n",
            "58/58 [==============================] - 1s 11ms/step - loss: 0.2745 - val_loss: 0.2777\n",
            "Epoch 289/300\n",
            "58/58 [==============================] - 0s 8ms/step - loss: 0.2703 - val_loss: 0.2781\n",
            "Epoch 290/300\n",
            "58/58 [==============================] - 0s 7ms/step - loss: 0.2702 - val_loss: 0.2753\n",
            "Epoch 291/300\n",
            "58/58 [==============================] - 0s 7ms/step - loss: 0.2701 - val_loss: 0.2755\n",
            "Epoch 292/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2700 - val_loss: 0.2762\n",
            "Epoch 293/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2703 - val_loss: 0.2762\n",
            "Epoch 294/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2705 - val_loss: 0.2752\n",
            "Epoch 295/300\n",
            "58/58 [==============================] - 0s 7ms/step - loss: 0.2701 - val_loss: 0.2773\n",
            "Epoch 296/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2708 - val_loss: 0.2788\n",
            "Epoch 297/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2708 - val_loss: 0.2769\n",
            "Epoch 298/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2712 - val_loss: 0.2754\n",
            "Epoch 299/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2699 - val_loss: 0.2753\n",
            "Epoch 300/300\n",
            "58/58 [==============================] - 0s 6ms/step - loss: 0.2696 - val_loss: 0.2752\n",
            "90/90 [==============================] - 1s 5ms/step\n",
            "104/104 [==============================] - 0s 2ms/step\n",
            "104/104 [==============================] - 1s 5ms/step\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (8,) + inhomogeneous part.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-1b6326bde65f>\u001b[0m in \u001b[0;36m<cell line: 48>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;31m# Convert the weights to a DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0mweights_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m# Save the weights DataFrame to an Excel file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    796\u001b[0m                     )\n\u001b[1;32m    797\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 798\u001b[0;31m                     mgr = ndarray_to_mgr(\n\u001b[0m\u001b[1;32m    799\u001b[0m                         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m                         \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mndarray_to_mgr\u001b[0;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0;31m# by definition an array here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0;31m# the dtypes will be coerced to a single dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prep_ndarraylike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy_on_sanitize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_dtype_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_prep_ndarraylike\u001b[0;34m(values, copy)\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;31m#  np.asarray would\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m         \u001b[0;31m# GH#21861 see test_constructor_list_of_lists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (8,) + inhomogeneous part."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Extract weights and biases for all layers\n",
        "layer_weights = []\n",
        "layer_biases = []\n",
        "for layer in model.layers:\n",
        "  # Exclude the first LSTM layer (not trainable)\n",
        "  if layer.name != 'lstm':\n",
        "    # Get weights (already NumPy arrays due to layer.get_weights())\n",
        "    weights = layer.get_weights()[0]\n",
        "    # Get biases (might need reshaping)\n",
        "    biases = layer.get_weights()[1].reshape(-1)  # Reshape if necessary (e.g., Dense layer)\n",
        "    layer_weights.append(weights)\n",
        "    layer_biases.append(biases)\n",
        "\n",
        "# Create DataFrames for weights and biases\n",
        "weights_dfs = []\n",
        "biases_dfs = []\n",
        "for i, (layer_weight, layer_bias) in enumerate(zip(layer_weights, layer_biases)):\n",
        "  # ... (similar DataFrame creation logic as before, considering layer names and column names)\n",
        "    # Ensure the DataFrame creation logic is indented within the loop\n",
        "\n",
        "# Combine and save to Excel (similar to previous code)\n",
        "final_df = pd.concat([combined_weights_df, combined_biases_df], axis=0)\n",
        "final_df.to_excel('lstm_weights_and_biases_from_finetuning.xlsx', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "TkhpjUPh2N3x",
        "outputId": "debcf3b4-bc57-4d5d-8a6a-9cb34da62668"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "expected an indented block after 'for' statement on line 17 (<ipython-input-40-e8bfdfba6b2f>, line 22)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-40-e8bfdfba6b2f>\"\u001b[0;36m, line \u001b[0;32m22\u001b[0m\n\u001b[0;31m    final_df = pd.concat([combined_weights_df, combined_biases_df], axis=0)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after 'for' statement on line 17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming your model has 8 layers (replace with actual number)\n",
        "num_layers = 8\n",
        "\n",
        "# Placeholder weights (replace with actual model weights retrieval)\n",
        "weights = []\n",
        "for _ in range(num_layers):  # Generate dummy weights for demonstration\n",
        "  weights.append(np.random.rand(10, 5))  # Example shape (10 rows, 5 columns)\n",
        "\n",
        "# Create a list to store weights as DataFrames (one DataFrame per layer)\n",
        "weights_dfs = []\n",
        "for i, layer_weights in enumerate(weights):\n",
        "  # Convert layer weights to NumPy array (already assumed in this example)\n",
        "  # weights_array = np.array(layer_weights)\n",
        "  # Create DataFrame for this layer's weights\n",
        "  layer_df = pd.DataFrame(layer_weights)\n",
        "  # Add a column for layer name (optional)\n",
        "  layer_df['Layer'] = i+1  # Add layer number (starting from 1)\n",
        "  # Append DataFrame to the list\n",
        "  weights_dfs.append(layer_df)\n",
        "\n",
        "# Combine all DataFrames into a single DataFrame for the Excel file\n",
        "# (This is the key modification)\n",
        "combined_df = pd.concat(weights_dfs, axis=1)\n",
        "\n",
        "# Ensure there are 8 columns (pad with empty DataFrames if necessary)\n",
        "if len(combined_df.columns) < num_layers:\n",
        "  for i in range(num_layers - len(combined_df.columns)):\n",
        "    empty_df = pd.DataFrame(columns=[f\"Layer_{i+len(combined_df.columns)+1}\"])\n",
        "    combined_df = pd.concat([combined_df, empty_df], axis=1)\n",
        "\n",
        "# Save the weights DataFrame to an Excel file\n",
        "combined_df.to_excel('lstm_weights_all_layers_single_sheet.xlsx', index=False)\n"
      ],
      "metadata": {
        "id": "irXgiAEzs7oM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load your trained LSTM model\n",
        "model = load_model('your_lstm_model.h5')\n",
        "\n",
        "# Get the weights of the model\n",
        "weights = model.get_weights()\n",
        "\n",
        "# Convert the weights to a numpy array\n",
        "weights_array = np.array(weights)\n",
        "\n",
        "# Create a DataFrame from the weights array\n",
        "weights_df = pd.DataFrame(weights_array)\n",
        "\n",
        "# Save the weights DataFrame to an Excel file\n",
        "weights_df.to_excel('lstm_weights.xlsx', index=False)\n"
      ],
      "metadata": {
        "id": "p3oAkTCqnA6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming your model has 8 layers (replace with actual number)\n",
        "num_layers = 8\n",
        "\n",
        "# Placeholder weights and biases (replace with actual model weights retrieval)\n",
        "weights = []\n",
        "biases = []\n",
        "for _ in range(num_layers):  # Generate dummy weights and biases for demonstration\n",
        "    weights.append(np.random.rand(10, 5))  # Example weight shape (10 rows, 5 columns)\n",
        "    biases.append(np.random.rand(5))  # Example bias shape (5 elements)\n",
        "\n",
        "# Create lists to store weights and biases as DataFrames\n",
        "weights_dfs = []\n",
        "biases_dfs = []\n",
        "for i, (layer_weights, layer_bias) in enumerate(zip(weights, biases)):\n",
        "    # Create DataFrames for weights and biases\n",
        "    weights_df = pd.DataFrame(layer_weights)\n",
        "    biases_df = pd.DataFrame(layer_bias)\n",
        "    # Add a column for layer name (optional)\n",
        "    weights_df['Layer'] = f\"W_{i+1}\"  # Weights with \"W_\" prefix\n",
        "    biases_df['Layer'] = f\"B_{i+1}\"  # Biases with \"B_\" prefix\n",
        "    # Append DataFrames to respective lists\n",
        "    weights_dfs.append(weights_df)\n",
        "    biases_dfs.append(biases_df)\n",
        "\n",
        "# Combine all weight DataFrames into a single DataFrame with MultiIndex\n",
        "combined_weights_df = pd.concat(weights_dfs, axis=1, keys=[f'W_{i+1}' for i in range(num_layers)])\n",
        "\n",
        "# Combine all bias DataFrames into a single DataFrame with MultiIndex\n",
        "combined_biases_df = pd.concat(biases_dfs, axis=1, keys=[f'B_{i+1}' for i in range(num_layers)])\n",
        "\n",
        "# Create the final DataFrame with weights and biases stacked vertically (weights on top)\n",
        "final_df = pd.concat([combined_weights_df, combined_biases_df], axis=0)\n",
        "\n",
        "# Save the final DataFrame to an Excel file with two sheets\n",
        "with pd.ExcelWriter('lstm_weights_and_biases_two_sheets.xlsx') as writer:\n",
        "    final_df.to_excel(writer, sheet_name='Weights_Biases')\n"
      ],
      "metadata": {
        "id": "LSZFMFgR0nOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import r2_score\n",
        "\n",
        "\n",
        "# Calculate R-squared for training, validation, and testing sets\n",
        "r2_train = r2_score(y_train, y_train_pred)\n",
        "r2_val = r2_score(y_val, y_val_pred)\n",
        "r2_test = r2_score(y_test, y_test_pred)\n",
        "\n",
        "print(f'R-squared for Training Set: {r2_train:.3f}')\n",
        "print(f'R-squared for Validation Set: {r2_val:.3f}')\n",
        "print(f'R-squared for Testing Set: {r2_test:.3f}')\n",
        "\n"
      ],
      "metadata": {
        "id": "-cmGR82K2yGg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3cbb06c-1ed0-41f0-92e8-57150f537426"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R-squared for Training Set: 0.907\n",
            "R-squared for Validation Set: 0.900\n",
            "R-squared for Testing Set: 0.903\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install h5py numpy pandas\n"
      ],
      "metadata": {
        "id": "lMw3C35di-Ys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load data from .h5 file\n",
        "data = pd.read_hdf('/content/lstm_model_weights.h5')\n",
        "\n",
        "# Save data to Excel\n",
        "data.to_excel('output.xlsx', index=False)\n"
      ],
      "metadata": {
        "id": "aLVp1z8IgsR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "\n",
        "# Open the HDF5 file\n",
        "with h5py.File('/content/lstm_model_weights.h5', 'r') as file:\n",
        "    # Print out all the keys in the HDF5 file\n",
        "    print(\"Keys in the HDF5 file:\")\n",
        "    for key in file.keys():\n",
        "        print(key)\n"
      ],
      "metadata": {
        "id": "qxbV2JjQh6Jj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import h5py\n",
        "\n",
        "# Load data from .h5 file\n",
        "with h5py.File('/content/lstm_model_weights.h5', 'r') as file:\n",
        "    # Check if the key exists\n",
        "    if 'dense_3' in file:\n",
        "        # Check if the key is a dataset\n",
        "        if isinstance(file['dense_3'], h5py.Dataset):\n",
        "            data = file['dense_3'][:]\n",
        "        else:\n",
        "            raise TypeError(\"The key 'dense_3' exists but is not a dataset.\")\n",
        "    else:\n",
        "        raise KeyError(\"The key 'dense_3' does not exist in the HDF5 file.\")\n",
        "\n",
        "# Convert to DataFrame\n",
        "data_df = pd.DataFrame(data)\n",
        "\n",
        "# Save data to Excel\n",
        "data_df.to_excel('output.xlsx', index=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "cZNqE3ayiVgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load data from .h5 file\n",
        "data = pd.read_hdf('/content/lstm_model_weights.h5', key='top_level_model_weights')\n",
        "\n",
        "# Save data to Excel\n",
        "data.to_excel('output.xlsx', index=False)\n"
      ],
      "metadata": {
        "id": "g4sO0Tw9htAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Replace 'your_h5_file.hdf5' with the actual filename\n",
        "filename = '/content/lstm_model_weights.h5'\n",
        "\n",
        "# Open the HDF5 file in read-only mode\n",
        "with h5py.File(filename, 'r') as f:\n",
        "\n",
        "    # Get the dataset you want to convert (replace 'your_dataset_name' with the actual name)\n",
        "\n",
        "    # Convert the dataset to a NumPy array\n",
        "    data = np.array(filename)\n",
        "\n",
        "    # Create a pandas DataFrame from the array\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Replace 'output.csv' with your desired output filename\n",
        "    df.to_csv('output.csv', index=False)\n",
        "\n",
        "print(\"Converted HDF5 file to CSV successfully!\")\n"
      ],
      "metadata": {
        "id": "Mj6rwSDsfmYK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}